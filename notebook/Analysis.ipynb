{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unlocking Church Growth: Data Insights from the National Congregations Study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Author**: Nathan Schaaf <br>\n",
    "**Date**: July 10, 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "Chaves, M. (2021, January 25). National Congregations Study, Cumulative Dataset (1998, 2006-2007, 2012, and 2018-2019). The Association of Religion Data Archives. doi:10.17605/OSF.IO/V5ZKB. https://www.thearda.com/data-archive?fid=NCSIV.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective / Thesis\n",
    "How can we increase our church attendance? In this analysis we explore the \"National Congregations Study, Cumuliative Dataset (1998, 2006-2007, 2012, and 2018-2019)\" to look for correlations between congregational offerings such as facilities, worship services, staffing, music, and programs and an increase congregational size. The dataset includes over 5,000 congregations sampled through in-person interviews in four periods between 1998 and 2019. This analysis is inteded for decision-makers within religious congregations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Understanding and Preparation\n",
    "Data Cleaning: Ensure the dataset is clean, dealing with missing values, and outliers. <br>\n",
    "Data Transformation: Create relevant variables, such as attendance growth rates, and standardize/normalize data if necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies\n",
    "Import libraries needed for this notebook. These may be used in a future requirements.txt file for modular project development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import requests\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "\n",
    "#from sklearn.model_selection import train_test_split\n",
    "#from sklearn.linear_model import LogisticRegression\n",
    "#from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "#from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Import DictionaryCollection class from Dictionaries.py\n",
    "    # Download the file from GitHub\n",
    "url = 'https://raw.githubusercontent.com/NRSchaaf/unlocking-church-growth/main/notebook/dictionaries.py'\n",
    "response = requests.get(url)\n",
    "\n",
    "    # Save the file locally\n",
    "with open('dictionaries.py', 'wb') as file:\n",
    "    file.write(response.content)\n",
    "from dictionaries import dictionaryCollection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, the data source altered all categorical data into ordinal values and all Boolean data into ‘1’ and ‘2’, yes/no respectively. The above dictionaries.py file implements the provided translational text documents from the data source by converting the ordinal values back into categorical data. It also converts the 2’s into 0 achieving standard Boolean representation to avoid confusion and modeling issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "url = 'https://raw.githubusercontent.com/NRSchaaf/unlocking-church-growth/main/dataset/dataset.csv'\n",
    "data = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the dataset directly from the GitHub repository can aid others in reviewing, replicating, or continuting this analysis. Unfortuantly, no APIs were avaialble to access the dataset directly from the source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dictionaryCollection' object has no attribute 'replace_numeric_values'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m dict_collection \u001b[38;5;241m=\u001b[39m dictionaryCollection()\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Replace numerical values column with string values from dictionaries\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mdict_collection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplace_numeric_values\u001b[49m(data)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'dictionaryCollection' object has no attribute 'replace_numeric_values'"
     ]
    }
   ],
   "source": [
    "# Initialize DictionaryCollection instance\n",
    "dict_collection = dictionaryCollection()\n",
    "\n",
    "# Replace numerical values column with string values from dictionaries\n",
    "data = dict_collection.replace_numeric_values(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a custom theme for all Seaborn visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seaborn_style(font_family, background_color, grid_color, text_color):\n",
    "    sns.set_style({\n",
    "        \"axes.facecolor\": background_color,\n",
    "        \"figure.facecolor\": background_color,\n",
    "\n",
    "        \"grid.color\": grid_color,\n",
    "        \"axes.edgecolor\": grid_color,\n",
    "        \"axes.grid\": True,\n",
    "        \"axes.axisbelow\": True,\n",
    "        \n",
    "        \"axes.labelcolor\": text_color,\n",
    "        \"text.color\": text_color,\n",
    "        \"font.family\": font_family,\n",
    "        \"xtick.color\": text_color,\n",
    "        \"ytick.color\": text_color,\n",
    "\n",
    "        \"xtick.bottom\": False,\n",
    "        \"xtick.top\": False,\n",
    "        \"ytick.left\": False,\n",
    "        \"ytick.right\": False,\n",
    "\n",
    "        \"axes.spines.left\": False,\n",
    "        \"axes.spines.bottom\": True,\n",
    "        \"axes.spines.right\": False,\n",
    "        \"axes.spines.top\": False,\n",
    "    }\n",
    ")\n",
    "\n",
    "background_color = \"#006064\"\n",
    "grid_color = \"#cccccc\"\n",
    "bar_color = \"#009688\"\n",
    "text_color = \"#ffffff\"\n",
    "font_family = \"Arial\"\n",
    "\n",
    "set_seaborn_style(font_family, background_color, grid_color, text_color)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The denomination is a key categorical feature of the dataset for the analysis reader. Religious beliefs can influence behaviors, values, lifestyle choice and practices of a congregation and its members. They can also play a crucial role in social interactions and cultural norms. Since there is over 60 denominations surveyed in this dataset, a custom color palette for denominations has been created below so each is represented by a consistent color in any visual analytics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom color palette for all denominations\n",
    "denominations = data['DENOM'].unique()\n",
    "colors = sns.color_palette('Set2', len(denominations))\n",
    "custom_palette = dict(zip(denominations, colors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new features based on conditions\n",
    "data['change_increase'] = np.where(data['CHANGE'].str.contains('Increased'), True, False)\n",
    "data['change_decrease'] = np.where(data['CHANGE'].str.contains('Decreased'), True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change column data types\n",
    "data['DENOM'] = data['DENOM'].astype('category')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select specific features\n",
    "selected_columns = ['DENOM', 'change_increase', 'change_decrease']\n",
    "df = data[selected_columns].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select specific features\n",
    "selected_columns = ['DENOM', 'BLDGTYPE', 'VIEWBLDG', 'REMODEL', 'BLDGYEAR', 'HOMESCHL', 'HAVESCHL', 'USEBLDG', 'PERMPURP', 'change_increase', 'change_decrease']\n",
    "df_facilities = data[selected_columns].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select specific features\n",
    "selected_columns = ['DENOM', 'SINGING', 'CHOIR', 'PIANO', 'ORGAN', 'DRUMS', 'ELECGTR', 'GUITAR', 'change_increase', 'change_decrease']\n",
    "df_music = data[selected_columns].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select specific features\n",
    "selected_columns = ['DENOM', 'FTSTAFF', 'PTSTAFF', 'FTSCHLD', 'PTSCHLD', 'FTSYOUTH', 'PTSYOUTH', 'FTSYA', 'PTSYA', 'FTSMUSIC', 'PTSMUSIC', 'FTSREDU', 'PTSREDU', 'FTSFAMIN', 'PTSFAMIN', 'FTSCARE', 'PTSCARE', 'FTSPSYCH', 'PTSPSYCH', 'FTSENGAG', 'PTSENGAG', 'FTSGROW', 'PTSGROW', 'FTSREACH', 'PTSREACH', 'FTSADMIN', 'PTSADMIN', 'FTSVOLC', 'PTSVOLC', 'FTSWTECH', 'PTSWTECH', 'FTSTECH', 'PTSTECH', 'FTSMEDIA', 'PTSMEDIA', 'change_increase', 'change_decrease']\n",
    "df_staffing = data[selected_columns].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select specific features\n",
    "selected_columns = ['DENOM', 'NUMSERV1', 'LENGTH', 'SERMON', 'SERMTIME', 'SPKRDWN', 'NUMSPOKE', 'GREET', 'KIDTIME', 'TEENPART', 'ROBE', 'APPLAUSE', 'LAUGH', 'PROGRAM', 'OVERHEAD', 'STREAMED', 'SMTPHONE', 'CONGREAD', 'OFFERING', 'FTSENGAG', 'SOCLTIME', 'change_increase', 'change_decrease']\n",
    "df_worship = data[selected_columns].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select specific features\n",
    "selected_columns = ['DENOM', 'CLSYACS', 'CLSADLT', 'YTHGRP', 'TEENCHOR', 'TEENCAMP', 'TEENVOL', 'POLITICS', 'DISBIBLE', 'BOOKS', 'PARENTS', 'VOTERREG', 'SCIENCE', 'ENVIRON', 'ORGVOLS', 'WORKPROB', 'NEWMEMS', 'TRAIN', 'RACEREL', 'OTHTRAD', 'OWNMONY', 'CONGMONY', 'ASSESS', 'MARRIAGE', 'WOMENGRP', 'MENGRP', 'EXERCISE', 'LGBT', 'change_increase', 'change_decrease']\n",
    "df_programs = data[selected_columns].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descriptive Analysis\n",
    "Summary Statistics: Compute basic statistics (mean, median, mode) for variables related to facilities, worship services, staffing, music, programs, and congregational size. <br>\n",
    "Distribution Analysis: Visualize the distributions of key variables to understand their spread and central tendency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Source Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Exploration\n",
    "data.info()\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of numerical features from select columns\n",
    "columns_of_interest = ['DENOM']\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.figure(figsize=(20, 6))\n",
    "\n",
    "for i, column in enumerate(columns_of_interest, 1):\n",
    "    plt.subplot(1, len(columns_of_interest), i)\n",
    "    sns.histplot(data[column], bins=20, kde=True)\n",
    "    plt.title(column)\n",
    "    plt.xlabel(\"Value\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.xticks(rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the occurrences of each unique value in 'DENOM' column\n",
    "denom_counts = data['DENOM'].value_counts()\n",
    "\n",
    "# Create a bar chart\n",
    "plt.figure(figsize=(20, 8))\n",
    "sns.barplot(x=denom_counts.index, y=denom_counts.values, palette=custom_palette, edgecolor=background_color)\n",
    "\n",
    "# Customize the plot\n",
    "plt.title('Denomination Sample Distribution', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Denomination', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Number of Congregations', fontweight='bold')\n",
    "plt.xticks(rotation=90)  # Rotate x-axis labels for better readability\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Facilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic exploration\n",
    "df_facilities.info()\n",
    "df_facilities.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Music"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Staffing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Worship"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Programs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA)\n",
    "Correlation Analysis: Use correlation matrices to identify relationships between congregational offerings and attendance.<br>\n",
    "Visualization: Create visualizations such as histograms, box plots, scatter plots, and heatmaps to explore patterns and trends."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inferential Analysis\n",
    "Hypothesis Testing: Conduct hypothesis tests (e.g., t-tests, ANOVA) to determine if there are statistically significant differences in attendance based on different offerings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Analysis\n",
    "Linear Regression: Model the relationship between congregational size (dependent variable) and offerings (independent variables) using linear regression.<br>\n",
    "Logistic Regression: If the outcome is categorical (e.g., increased attendance vs. no increase), use logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate Analysis\n",
    "Multiple Regression: Extend linear regression to include multiple independent variables to account for the effect of various factors simultaneously.<br>\n",
    "Principal Component Analysis (PCA): Reduce dimensionality and identify key factors contributing to attendance changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Series Analysis\n",
    "Trend Analysis: Examine how congregational size and offerings have changed over time.<br>\n",
    "Seasonality: Identify seasonal patterns in attendance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictive Analysis\n",
    "Machine Learning Models: Implement machine learning models (e.g., decision trees, random forests, gradient boosting) to predict attendance based on congregational offerings.<br>\n",
    "Model Evaluation: Use cross-validation and performance metrics (e.g., RMSE, AUC) to assess model accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Causal Analysis\n",
    "Causal Inference: Use methods like propensity score matching or instrumental variables to identify causal relationships between offerings and attendance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommendations\n",
    "Actionable Insights: Provide recommendations based on the analysis, highlighting which factors most significantly influence attendance.<br>\n",
    "Scenario Analysis: Simulate different scenarios to predict the impact of changes in offerings on attendance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle Missing Values\n",
    "missing_values = data.isnull().sum()\n",
    "print(\"\\nMissing Values:\")\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print Dataset\n",
    "print(\"\\nData Set:\")\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all features\n",
    "columns_list = data.columns.tolist()\n",
    "print(\"List of all columns:\")\n",
    "print(columns_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce CHANGE dimensionality from 5 to 3\n",
    "\n",
    "# Define replacement mapping\n",
    "replacement_mapping = {\n",
    "    'Decreased more than 10 percent': 'Decreased',\n",
    "    'Decreased less than 10 percent': 'Decreased',\n",
    "    'Increased less than 10 percent': 'Increased',\n",
    "    'Increased more than 10 percent': 'Increased',\n",
    "    'Same': 'Same'\n",
    "}\n",
    "\n",
    "# Replace values in 'CHANGE' column according to the mapping\n",
    "data['CHANGE'] = data['CHANGE'].replace(replacement_mapping)\n",
    "\n",
    "\n",
    "# Print the updated DataFrame to verify\n",
    "print(data.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['CHANGE'].unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target Variable Identification\n",
    "target_variables = ['ADLTCHG', 'CHANGE']\n",
    "target_variables_df=data[target_variables]\n",
    "print(target_variables_df.info())\n",
    "print(target_variables_df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seaborn Graph/Chart Theme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About Denominations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Church/Facility Features Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of features to include\n",
    "bc_columns = ['DENOM','REGION','BLDGTYPE','VIEWBLDG','REMODEL','CHANGE']\n",
    "bc_df = data[bc_columns].copy()\n",
    "\n",
    "# drop all empty rows under CHANGE feature\n",
    "bc_df.dropna(subset=['CHANGE'], inplace=True)\n",
    "\n",
    "print(bc_df.head())\n",
    "print()\n",
    "print('Shape: ', bc_df.shape)\n",
    "print()\n",
    "print(bc_df.describe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bc_df['CHANGE'].unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a cross-tabulation (contingency table) of DENOM vs CHANGE\n",
    "cross_table = pd.crosstab(data['DENOM'], data['CHANGE'])\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(30, 20))\n",
    "sns.heatmap(cross_table, cmap='coolwarm', annot=True, fmt='d')\n",
    "plt.title('Frequency of Denomination vs Change')\n",
    "plt.xlabel('Change')\n",
    "plt.ylabel('Denomination')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Differences in the CHANGE column\n",
    "\n",
    "# Calculate counts of each category by DENOM\n",
    "change_counts = pd.crosstab(data['DENOM'], data['CHANGE'], dropna=False)\n",
    "\n",
    "# Calculate proportions\n",
    "change_counts['Total'] = change_counts.sum(axis=1)\n",
    "change_counts_prop = change_counts.div(change_counts['Total'], axis=0)\n",
    "\n",
    "# Calculate differences\n",
    "change_counts_prop['Increase vs Decrease + Same'] = (\n",
    "    change_counts_prop['Increased'] - \n",
    "    (change_counts_prop['Decreased'] + change_counts_prop['Remained about the same'])\n",
    ")\n",
    "change_counts_prop['Decrease vs Increase + Same'] = (\n",
    "    change_counts_prop['Decreased'] - \n",
    "    (change_counts_prop['Increased'] + change_counts_prop['Remained about the same'])\n",
    ")\n",
    "\n",
    "# Sort by 'Increase vs Decrease + Same' difference in descending order\n",
    "change_counts_prop = change_counts_prop.sort_values(by='Increase vs Decrease + Same', ascending=False)\n",
    "\n",
    "# Reset index for plotting\n",
    "change_counts_prop = change_counts_prop.reset_index()\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(40, 10))\n",
    "sns.barplot(data=change_counts_prop, x='DENOM', y='Increase vs Decrease + Same', palette=custom_palette, label='Increase vs Decrease + Same')\n",
    "sns.barplot(data=change_counts_prop, x='DENOM', y='Decrease vs Increase + Same', palette=custom_palette, label='Decrease vs Increase + Same')\n",
    "plt.axhline(0, color='k', linestyle='--')  # Add a horizontal line at y=0\n",
    "plt.xlabel('Denomination')\n",
    "plt.ylabel('Difference in Congregation Size (%)')\n",
    "plt.title('Net Change in Congregation Size by Denomination', fontsize=20, fontweight='bold')  # Adjust title font size and weight)\n",
    "#plt.legend()\n",
    "plt.xticks(rotation=90) # Rotate x-axis labels vertically\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select top 15 denominations with the largest positive change\n",
    "top_15_denoms = change_counts_prop.head(15)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=top_15_denoms, x='DENOM', y='Increase vs Decrease + Same', palette=custom_palette)\n",
    "plt.axhline(0, color='k', linestyle='--')  # Add a horizontal line at y=0\n",
    "plt.xlabel('Denomination')\n",
    "plt.ylabel('Difference in Congregation Size (%)')\n",
    "plt.title('Top 15 Denominations with Largest Net Increase in Congregation Size', fontweight='bold')\n",
    "plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for better readability\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the DataFrame for rows where 'CHANGE' is \"Increased\"\n",
    "increased_data = data[data['CHANGE'] == 'Increased']\n",
    "\n",
    "# Count occurrences of each denomination\n",
    "denom_counts = increased_data['DENOM'].value_counts()\n",
    "\n",
    "# Select the top 15 denominations\n",
    "top_15_denoms = denom_counts.head(15)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=top_15_denoms.index, y=top_15_denoms.values, palette=custom_palette)\n",
    "plt.xlabel('Denomination')\n",
    "plt.ylabel('Number of Increased Congregations')\n",
    "plt.title('Largest 15 Denominations with Increased Congregations', fontweight='bold')\n",
    "plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for better readability\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply One Hot Encoding to 'CHANGE' feature\n",
    "encoder = OneHotEncoder(drop='first', sparse_output=False)  # Drop first category and get a dense array\n",
    "encoded_change = encoder.fit_transform(bc_df[['CHANGE']])\n",
    "\n",
    "# Get feature names for encoded columns\n",
    "encoded_columns = encoder.get_feature_names_out(['CHANGE'])\n",
    "\n",
    "# Create DataFrame from encoded columns\n",
    "encoded_change_df = pd.DataFrame(encoded_change, columns=encoded_columns)\n",
    "\n",
    "# Reset index of bc_df and encoded_change_df for proper concatenation\n",
    "bc_df.reset_index(drop=True, inplace=True)\n",
    "encoded_change_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Concatenate encoded columns to original DataFrame and drop the original 'CHANGE' column\n",
    "bc_df_encoded = pd.concat([bc_df, encoded_change_df], axis=1).drop(['CHANGE'], axis=1)\n",
    "\n",
    "# Print the shape and statistics of the DataFrame\n",
    "print('Shape: ', bc_df_encoded.shape)\n",
    "print()\n",
    "print(bc_df_encoded.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform One-Hot Encoding\n",
    "denom_dummies = pd.get_dummies(bc_df_encoded['DENOM'], prefix='DENOM')\n",
    "\n",
    "# Concatenate the encoded columns to the original DataFrame\n",
    "bc_df_encoded = pd.concat([bc_df_encoded, denom_dummies], axis=1)\n",
    "\n",
    "# Drop the original 'DENOM' column\n",
    "bc_df_encoded.drop(['DENOM'], axis=1, inplace=True)\n",
    "\n",
    "# Print the head of the DataFrame to verify\n",
    "print(bc_df_encoded.head())\n",
    "print()\n",
    "print('Shape: ', bc_df_encoded.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlation matrix\n",
    "correlation_matrix = bc_df_encoded.corr()\n",
    "\n",
    "# Create a heatmap\n",
    "plt.figure(figsize=(100, 80))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", vmin=-1, vmax=1)\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Worship Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of features to include\n",
    "bw_columns = ['NUMSERV1','LENGTH','SERMON','SERMTIME','SPKRDWN','MUSICMIN','GREET','ROBE','LAUGH','PROGRAM','CHANGE']\n",
    "bW_df = data[bw_columns].copy()\n",
    "print(bW_df.head())\n",
    "print(bW_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of NaN values in each column\n",
    "nan_counts = bW_df.isna().sum()\n",
    "print(\"\\nNumber of NaN values in each column:\")\n",
    "print(nan_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "# Drop NaN values from 'CHANGE' columns\n",
    "bw_df_clean = bW_df.dropna(subset=['CHANGE'])\n",
    "\n",
    "# Replace NaN values in 'SPKRDWN' and 'ROBE' columns with 2\n",
    "bw_df_clean.loc[:, ['SPKRDWN', 'ROBE', 'GREET', 'LAUGH']] = bw_df_clean.loc[:, ['SPKRDWN', 'ROBE', 'GREET', 'LAUGH']].fillna(2)\n",
    "\n",
    "# Replace NaN values in 'NUMSERV1', 'LENGTH', 'SERMON', 'SERMTIME', 'MUSICMIN', 'GREET', 'PROGRAM' columns with the mean from each column\n",
    "columns_to_replace_with_mean = ['NUMSERV1', 'LENGTH', 'SERMTIME', 'MUSICMIN']\n",
    "mean_values = bw_df_clean[columns_to_replace_with_mean].mean()\n",
    "bw_df_clean.loc[:, columns_to_replace_with_mean] = bw_df_clean.loc[:, columns_to_replace_with_mean].fillna(mean_values)\n",
    "\n",
    "# List of columns where you want to replace the value 2 with 0\n",
    "columns_to_replace = ['SERMON', 'SPKRDWN', 'GREET', 'ROBE', 'LAUGH', 'PROGRAM']\n",
    "# Replace the value 2 with 0 in the specified columns\n",
    "bw_df_clean.loc[:, columns_to_replace] = bw_df_clean.loc[:, columns_to_replace].replace(2, 0)\n",
    "\n",
    "nan_counts = bw_df_clean.isna().sum()\n",
    "print(\"\\nNumber of NaN values in each column after preprocessing:\")\n",
    "print(nan_counts)\n",
    "print(bw_df_clean.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "bw_df_standardized = pd.DataFrame(scaler.fit_transform(bw_df_clean), columns=bw_df_clean.columns)\n",
    "print(bw_df_standardized.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a correlation matrix\n",
    "correlation_matrix = bw_df_standardized.corr()\n",
    "# Plotting the correlation heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns to include in the scatter plot\n",
    "columns_to_plot = ['NUMSERV1', 'LENGTH', 'SERMON']\n",
    "\n",
    "# Scatter plot of specified columns vs 'CHANGE'\n",
    "plt.figure(figsize=(10, 6))\n",
    "for column in columns_to_plot:\n",
    "    plt.scatter(bw_df_standardized[column], bw_df_standardized['CHANGE'], label=column, alpha=0.5)\n",
    "\n",
    "plt.title('Scatter Plots of NUMSERV1, LENGTH, and SERMON vs CHANGE')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('CHANGE')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove outliers using IQR\n",
    "def remove_outliers(bw_df_standardized, columns):\n",
    "    for column in columns:\n",
    "        Q1 = bw_df_standardized[column].quantile(0.25)\n",
    "        Q3 = bw_df_standardized[column].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        bw_df_standardized = bw_df_standardized[(bw_df_standardized[column] >= lower_bound) & (bw_df_standardized[column] <= upper_bound)]\n",
    "    return bw_df_standardized\n",
    "\n",
    "# List of columns from which to remove outliers\n",
    "columns_to_check = ['NUMSERV1', 'LENGTH', 'SERMON']\n",
    "\n",
    "# Remove outliers from the specified columns\n",
    "bw_df_standardized = remove_outliers(bw_df_standardized, columns_to_check)\n",
    "\n",
    "# Verify the shape of the cleaned DataFrame\n",
    "print(bw_df_standardized.shape)\n",
    "\n",
    "# Scatter plot of cleaned specified columns vs 'CHANGE'\n",
    "plt.figure(figsize=(10, 6))\n",
    "for column in columns_to_check:\n",
    "    plt.scatter(bw_df_standardized[column], bw_df_standardized['CHANGE'], label=column, alpha=0.5)\n",
    "\n",
    "plt.title('Scatter Plots of NUMSERV1, LENGTH, and SERMON vs CHANGE (After Removing Outliers)')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('CHANGE')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns to plot\n",
    "columns_to_plot = ['NUMSERV1', 'LENGTH', 'SERMON']\n",
    "\n",
    "# Create boxplots for the specified columns\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "for i, column in enumerate(columns_to_plot, 1):\n",
    "    plt.subplot(1, 3, i)\n",
    "    sns.boxplot(y=bw_df_standardized[column])\n",
    "    plt.title(f'Boxplot of {column}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the data is clean and ready for modeling\n",
    "columns_to_use = ['NUMSERV1', 'LENGTH'] #, 'SERMON', 'SPKRDWN', 'ROBE', 'GREET', 'LAUGH'\n",
    "X = bw_df_standardized[columns_to_use]\n",
    "\n",
    "# Convert 'CHANGE' to categorical if it's not already\n",
    "bw_df_standardized['CHANGE'] = bw_df_standardized['CHANGE'].astype('int').astype('category')\n",
    "y = bw_df_standardized['CHANGE']\n",
    "\n",
    "# Check class distribution\n",
    "print(\"Class distribution in 'CHANGE':\")\n",
    "print(y.value_counts())\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Initialize and train the logistic regression model\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "class_report = classification_report(y_test, y_pred, zero_division=1)\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Confusion Matrix:\\n{conf_matrix}')\n",
    "print(f'Classification Report:\\n{class_report}')\n",
    "\n",
    "# Plotting the confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=model.classes_, yticklabels=model.classes_)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply SMOTE to balance the classes\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "# Split the resampled data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42, stratify=y_resampled)\n",
    "\n",
    "# Initialize and train the logistic regression model with class weights\n",
    "model = LogisticRegression(max_iter=1000, class_weight='balanced')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "class_report = classification_report(y_test, y_pred, zero_division=1)\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Confusion Matrix:\\n{conf_matrix}')\n",
    "print(f'Classification Report:\\n{class_report}')\n",
    "\n",
    "# Plotting the confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=model.classes_, yticklabels=model.classes_)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Initialize and train the Random Forest model with class weights\n",
    "rf_model = RandomForestClassifier(class_weight='balanced', random_state=42)\n",
    "\n",
    "# Define a parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "# Use GridSearchCV to find the best hyperparameters\n",
    "grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best estimator\n",
    "best_rf_model = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_rf = best_rf_model.predict(X_test)\n",
    "\n",
    "# Evaluate the Random Forest model\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "conf_matrix_rf = confusion_matrix(y_test, y_pred_rf)\n",
    "class_report_rf = classification_report(y_test, y_pred_rf, zero_division=1)\n",
    "\n",
    "print(f'Best Random Forest Model: {best_rf_model}')\n",
    "print(f'Accuracy: {accuracy_rf}')\n",
    "print(f'Confusion Matrix:\\n{conf_matrix_rf}')\n",
    "print(f'Classification Report:\\n{class_report_rf}')\n",
    "\n",
    "# Plotting the confusion matrix for Random Forest\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix_rf, annot=True, fmt='d', cmap='Blues', xticklabels=best_rf_model.classes_, yticklabels=best_rf_model.classes_)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix - Random Forest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importances from the trained Random Forest model\n",
    "feature_importances = best_rf_model.feature_importances_\n",
    "importance_df = pd.DataFrame({'Feature': columns_to_use, 'Importance': feature_importances})\n",
    "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(importance_df)\n",
    "\n",
    "# Plotting feature importances\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Importance', y='Feature', data=importance_df)\n",
    "plt.title('Feature Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the average value of 'NUMSERV1', 'LENGTH'\n",
    "average_numserv1 = bw_df_standardized['NUMSERV1'].mean()\n",
    "average_length = bw_df_standardized['LENGTH'].mean()\n",
    "\n",
    "# Define the three scenarios for 'NUMSERV1' and 'LENGTH'\n",
    "scenarios = ['Average', 'Twice Average', 'Half Average']\n",
    "numserv1_values = [average_numserv1, 2 * average_numserv1, 0.5 * average_numserv1]\n",
    "length_values = [average_length, 2 * average_length, 0.5 * average_length]\n",
    "\n",
    "# Predict 'CHANGE' values for each scenario using the trained Random Forest model\n",
    "change_values = []\n",
    "for numserv1, length in zip(numserv1_values, length_values):\n",
    "    # Predict 'CHANGE' using the trained Random Forest model\n",
    "    predicted_change = best_rf_model.predict([[numserv1, length]])\n",
    "    change_values.append(predicted_change[0])  # Append the predicted 'CHANGE' value to the list\n",
    "\n",
    "# Create a bar chart for 'NUMSERV1'\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(scenarios, change_values, color='skyblue')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('NUMSERV1 Scenarios')\n",
    "plt.ylabel('Predicted CHANGE')\n",
    "plt.title('Impact of NUMSERV1 on Predicted CHANGE (Random Forest Model)')\n",
    "\n",
    "# Show plot\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Repeat for 'LENGTH'\n",
    "# Predict 'CHANGE' values for each scenario using the trained Random Forest model\n",
    "change_values = []\n",
    "for numserv1, length in zip(numserv1_values, length_values):\n",
    "    # Predict 'CHANGE' using the trained Random Forest model\n",
    "    predicted_change = best_rf_model.predict([[numserv1, length]])\n",
    "    change_values.append(predicted_change[0])  # Append the predicted 'CHANGE' value to the list\n",
    "\n",
    "# Create a bar chart for 'LENGTH'\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(scenarios, change_values, color='salmon')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('LENGTH Scenarios')\n",
    "plt.ylabel('Predicted CHANGE')\n",
    "plt.title('Impact of LENGTH on Predicted CHANGE (Random Forest Model)')\n",
    "\n",
    "# Show plot\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the median 'LENGTH' and 'NUMSERV1'\n",
    "median_length = data['LENGTH'].median()\n",
    "median_numserv1 = data['NUMSERV1'].median()\n",
    "\n",
    "print(f\"Median LENGTH: {median_length}\")\n",
    "print(f\"Median NUMSERV1: {median_numserv1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the increase scenarios for 'NUMSERV1' and 'LENGTH'\n",
    "numserv1_increase = 2 * average_numserv1\n",
    "length_increase = 2 * average_length\n",
    "\n",
    "# Predict probabilities for 'CHANGE' > 0.25 and 'CHANGE' > 0.5 given the increase in 'NUMSERV1' and 'LENGTH'\n",
    "probabilities_nums = best_rf_model.predict_proba([[numserv1_increase, average_length]])\n",
    "prob_gt_025_nums = sum(prob for prob in probabilities_nums[0] if prob > 0.25)\n",
    "prob_gt_05_nums = sum(prob for prob in probabilities_nums[0] if prob > 0.5)\n",
    "\n",
    "probabilities_length = best_rf_model.predict_proba([[average_numserv1, length_increase]])\n",
    "prob_gt_025_length = sum(prob for prob in probabilities_length[0] if prob > 0.25)\n",
    "prob_gt_05_length = sum(prob for prob in probabilities_length[0] if prob > 0.5)\n",
    "\n",
    "# Display the probabilities\n",
    "print(f\"Probability of 'CHANGE' > 0.25 with increased NUMSERV1: {prob_gt_025_nums}\")\n",
    "print(f\"Probability of 'CHANGE' > 0.5 with increased NUMSERV1: {prob_gt_05_nums}\")\n",
    "\n",
    "print(f\"Probability of 'CHANGE' > 0.25 with increased LENGTH: {prob_gt_025_length}\")\n",
    "print(f\"Probability of 'CHANGE' > 0.5 with increased LENGTH: {prob_gt_05_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the decrease scenarios for 'NUMSERV1' and 'LENGTH'\n",
    "numserv1_decrease = 0.5 * average_numserv1\n",
    "length_decrease = 0.5 * average_length\n",
    "\n",
    "# Predict probabilities for 'CHANGE' > 0.25 and 'CHANGE' > 0.5 given the decrease in 'NUMSERV1' and 'LENGTH'\n",
    "probabilities_nums_decrease = best_rf_model.predict_proba([[numserv1_decrease, average_length]])\n",
    "prob_gt_025_nums_decrease = sum(prob for prob in probabilities_nums_decrease[0] if prob > 0.25)\n",
    "prob_gt_05_nums_decrease = sum(prob for prob in probabilities_nums_decrease[0] if prob > 0.5)\n",
    "\n",
    "probabilities_length_decrease = best_rf_model.predict_proba([[average_numserv1, length_decrease]])\n",
    "prob_gt_025_length_decrease = sum(prob for prob in probabilities_length_decrease[0] if prob > 0.25)\n",
    "prob_gt_05_length_decrease = sum(prob for prob in probabilities_length_decrease[0] if prob > 0.5)\n",
    "\n",
    "# Display the probabilities\n",
    "print(f\"Probability of 'CHANGE' > 0.25 with decreased NUMSERV1: {prob_gt_025_nums_decrease}\")\n",
    "print(f\"Probability of 'CHANGE' > 0.5 with decreased NUMSERV1: {prob_gt_05_nums_decrease}\")\n",
    "\n",
    "print(f\"Probability of 'CHANGE' > 0.25 with decreased LENGTH: {prob_gt_025_length_decrease}\")\n",
    "print(f\"Probability of 'CHANGE' > 0.5 with decreased LENGTH: {prob_gt_05_length_decrease}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the increase scenarios for 'NUMSERV1' and 'LENGTH'\n",
    "numserv1_increase = 2 * average_numserv1\n",
    "length_increase = 2 * average_length\n",
    "\n",
    "# Predict probabilities for 'CHANGE' > 0.25 and 'CHANGE' > 0.5 given the increase in 'NUMSERV1' and 'LENGTH'\n",
    "probabilities_nums_increase = best_rf_model.predict_proba([[numserv1_increase, average_length]])\n",
    "prob_gt_025_nums_increase = sum(prob for prob in probabilities_nums_increase[0] if prob > 0.25)\n",
    "prob_gt_05_nums_increase = sum(prob for prob in probabilities_nums_increase[0] if prob > 0.5)\n",
    "\n",
    "probabilities_length_increase = best_rf_model.predict_proba([[average_numserv1, length_increase]])\n",
    "prob_gt_025_length_increase = sum(prob for prob in probabilities_length_increase[0] if prob > 0.25)\n",
    "prob_gt_05_length_increase = sum(prob for prob in probabilities_length_increase[0] if prob > 0.5)\n",
    "\n",
    "# Display the probabilities\n",
    "print(f\"Probability of 'CHANGE' > 0.25 with increased NUMSERV1: {prob_gt_025_nums_increase}\")\n",
    "print(f\"Probability of 'CHANGE' > 0.5 with increased NUMSERV1: {prob_gt_05_nums_increase}\")\n",
    "\n",
    "print(f\"Probability of 'CHANGE' > 0.25 with increased LENGTH: {prob_gt_025_length_increase}\")\n",
    "print(f\"Probability of 'CHANGE' > 0.5 with increased LENGTH: {prob_gt_05_length_increase}\")\n",
    "# Define the decrease scenarios for 'NUMSERV1' and 'LENGTH'\n",
    "numserv1_decrease = 0.5 * average_numserv1\n",
    "length_decrease = 0.5 * average_length\n",
    "\n",
    "# Predict probabilities for 'CHANGE' > 0.25 and 'CHANGE' > 0.5 given the decrease in 'NUMSERV1' and 'LENGTH'\n",
    "probabilities_nums_decrease = best_rf_model.predict_proba([[numserv1_decrease, average_length]])\n",
    "prob_gt_025_nums_decrease = sum(prob for prob in probabilities_nums_decrease[0] if prob > 0.25)\n",
    "prob_gt_05_nums_decrease = sum(prob for prob in probabilities_nums_decrease[0] if prob > 0.5)\n",
    "\n",
    "probabilities_length_decrease = best_rf_model.predict_proba([[average_numserv1, length_decrease]])\n",
    "prob_gt_025_length_decrease = sum(prob for prob in probabilities_length_decrease[0] if prob > 0.25)\n",
    "prob_gt_05_length_decrease = sum(prob for prob in probabilities_length_decrease[0] if prob > 0.5)\n",
    "\n",
    "# Display the probabilities\n",
    "print(f\"Probability of 'CHANGE' > 0.25 with decreased NUMSERV1: {prob_gt_025_nums_decrease}\")\n",
    "print(f\"Probability of 'CHANGE' > 0.5 with decreased NUMSERV1: {prob_gt_05_nums_decrease}\")\n",
    "\n",
    "print(f\"Probability of 'CHANGE' > 0.25 with decreased LENGTH: {prob_gt_025_length_decrease}\")\n",
    "print(f\"Probability of 'CHANGE' > 0.5 with decreased LENGTH: {prob_gt_05_length_decrease}\")\n",
    "\n",
    "# Define the scenarios\n",
    "scenarios = ['Decrease', 'No Change', 'Increase']\n",
    "\n",
    "# Define the probabilities for 'CHANGE' > 0.25 and 'CHANGE' > 0.5 for NUMSERV1\n",
    "prob_gt_025_nums = [prob_gt_025_nums_decrease, prob_gt_025_nums, prob_gt_025_nums_increase]\n",
    "prob_gt_05_nums = [prob_gt_05_nums_decrease, prob_gt_05_nums, prob_gt_05_nums_increase]\n",
    "\n",
    "# Define the probabilities for 'CHANGE' > 0.25 and 'CHANGE' > 0.5 for LENGTH\n",
    "prob_gt_025_length = [prob_gt_025_length_decrease, prob_gt_025_length, prob_gt_025_length_increase]\n",
    "prob_gt_05_length = [prob_gt_05_length_decrease, prob_gt_05_length, prob_gt_05_length_increase]\n",
    "\n",
    "# Create bar plots for NUMSERV1\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(scenarios, prob_gt_025_nums, label='> 0.25', color='skyblue')\n",
    "plt.bar(scenarios, prob_gt_05_nums, label='> 0.5', color='salmon', alpha=0.5)\n",
    "plt.xlabel('NUMSERV1 Scenarios')\n",
    "plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bar plots for LENGTH\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.bar(scenarios, prob_gt_025_length, label='> 0.25', color='skyblue')\n",
    "plt.bar(scenarios, prob_gt_05_length, label='> 0.5', color='salmon', alpha=0.5)\n",
    "plt.xlabel('LENGTH Scenarios')\n",
    "plt.ylabel('Probability')\n",
    "plt.title('Change in Probability with LENGTH')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bar plots for NUMSERV1\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(scenarios, prob_gt_025_nums, label='> 0.25', color='skyblue')\n",
    "plt.bar(scenarios, prob_gt_05_nums, label='> 0.5', color='salmon', alpha=0.5)\n",
    "plt.xlabel('NUMSERV1 Scenarios')\n",
    "plt.ylabel('Probability')\n",
    "plt.title('Change in Probability with NUMSERV1')\n",
    "\n",
    "# Create bar plots for LENGTH\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.bar(scenarios, prob_gt_025_length, label='> 0.25', color='skyblue')\n",
    "plt.bar(scenarios, prob_gt_05_length, label='> 0.5', color='salmon', alpha=0.5)\n",
    "plt.xlabel('LENGTH Scenarios')\n",
    "plt.ylabel('Probability')\n",
    "plt.title('Change in Probability with LENGTH')\n",
    "\n",
    "# Add a common legend\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Musical Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
